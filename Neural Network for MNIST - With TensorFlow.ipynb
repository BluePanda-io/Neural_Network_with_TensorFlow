{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network for MNIST Data Set\n",
    "\n",
    "#### Classify 10 Different Numbers without TensorFlow\n",
    "In this NoteBook we will create a nerual network that can classify HandWriten Images in 10 different categories, first we will start by importing the libraries for using MNIST\n",
    "\n",
    "MNIST is a Data set that is udes specifically for people that start right now with Machine Learning and Neural Network and want to build something without having to worry about data sets regularization of the data and in general the input of the algorithm\n",
    "\n",
    "This dataset will provide 6000 images all of them with size $[28,28]$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the MNIST DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to use *mnist* in order to save in different variables the images and labels of the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = mnist.train_images()\n",
    "train_labels = mnist.train_labels()\n",
    "\n",
    "test_images = mnist.test_images()\n",
    "test_labels = mnist.test_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is really important to transform every image into a vector with $28*28 = 784$ features, this enable as to use this images - pixels as idividuals features that will be used as input in the Nueral Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)\n",
    "num_Imag = train_images.shape[0]\n",
    "nH = train_images.shape[1]\n",
    "nW = train_images.shape[2]\n",
    "Tr_Im = train_images.reshape((num_Imag,nH*nW))\n",
    "Tr_lab = train_labels\n",
    "Tr_Im = Tr_Im/255\n",
    "print(Tr_Im.shape)\n",
    "m=num_Imag # This is the number of input images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need to Change Y ( train_labels) from Hot-Vector to Binary representation \n",
    "\n",
    "Example of a Hot Vector:\n",
    "$$Y = [0,2,1,3]$$\n",
    "Example of the Binary representation\n",
    "$$ Y = \n",
    "\\begin{bmatrix}\n",
    "    1&0&0&0\\\\\n",
    "    0&0&1&0\\\\\n",
    "    0&1&0&0\\\\\n",
    "    0&0&0&1\\\\\n",
    "  \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HotVector(targets,n_labels):\n",
    "    Ytr = np.zeros((targets.shape[0], n_labels)) #empty one-hot matrix\n",
    "    Ytr[np.arange(targets.shape[0]), targets] = 1 #set target idx to 1\n",
    "\n",
    "    Ytr=np.transpose(Ytr)\n",
    "\n",
    "    print(Ytr[:,0:5]) # We will show only the first 5 results\n",
    "\n",
    "    print(Ytr.shape)\n",
    "    return Ytr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9]\n",
      "(60000,)\n",
      "[[0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n",
      "(10, 60000)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "n_labels = 10 # How many labels we have (10 different numbers)\n",
    "\n",
    "\n",
    "targets = train_labels\n",
    "print(targets[0:5]) # We will show only the first 5 results\n",
    "print(targets.shape)\n",
    "Ytr = HotVector(targets,n_labels)\n",
    "Ytr=Ytr.T\n",
    "print(Ytr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow\n",
    "\n",
    "TensorFlow is a Framework that let you build a Neural Network really fast and efficiently. Essential you only have to develop the Forward Propagation and the rest will be taken care of by the Framework.\n",
    "\n",
    "First of all, TensorFlow have Variables and **placeholders**, you will use placeholders in the input like X and Y. This is basically gives you the ability to initialize that there will be X which is the input but you will specialize the input later.\n",
    "\n",
    "$$X = tf.placeholder(tf.float32, [None, 784])$$\n",
    "\n",
    "This means that you will feed this Neural Network with a matrix that has dimensions $[Anything,784]$, so in the first dimension you will have any number but in the second one you need to have 784. This happens because you have the ability to feed the Neural Network with as many pictures as you want, but the pixels of this images need to be 784.\n",
    "\n",
    "The **Variable** is mainly used by the Weights and Biases, when something is denoted by Variable it means that they have an initial value but this value is changing while the ANN is learning.\n",
    "\n",
    "$$W = tf.Variable(tf.zeros([784, 10]))$$\n",
    "\n",
    "In the Variable you need to specialize the dimenisons and in this particular one you will going to have $[784,10]$ which is 784 nodes and then 10 nodes.\n",
    "\n",
    "Finally it is really important to initialize this variables, and you can do this with this command line.\n",
    "\n",
    "$$init = tf.global_variables_initializer()$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model of the Neural Netowrk\n",
    "\n",
    "Now that you have initialize the Variables and Placeholders of the Neural Network you need to specify the model that will be used. In this particular notebook we will use the simplest possible neural network \n",
    "\n",
    "\n",
    "$$Z = X*W + b$$\n",
    "\n",
    "\\begin{equation*}\n",
    "Z = \n",
    "  \\begin{bmatrix}\n",
    "    2&100& \\dotsc & 32\\\\\n",
    "    23&43& \\dotsc & 20\\\\\n",
    "    122&23& \\dotsc & 61\\\\\n",
    "    244&44& \\dotsc & 22\\\\\n",
    "    \\vdots & \\vdots  & \\vdots &  \\vdots \\\\\n",
    "    112&20& \\dotsc & 1 \\\\\n",
    "  \\end{bmatrix}\n",
    "  \\begin{bmatrix}\n",
    "    w_{00} & w_{10} & w_{20} & w_{30} & \\dotsc & w_{m0} \\\\\n",
    "    w_{01}& w_{11} & w_{21} & w_{31} & \\dotsc & w_{m1}\\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "    w_{0n} & w_{1n} & w_{2n} & w_{3n} & \\dotsc & w_{mn} \\\\\n",
    "  \\end{bmatrix}\n",
    "  +\n",
    "  \\begin{bmatrix}\n",
    "    b_{0} &\n",
    "    b_{1} &\n",
    "    \\dotsc &\n",
    "    b_{n} \n",
    "  \\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "<br><center> $ [60000,10] = [60000,784]*[784,10] + [1,10] $ </center></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation with SoftMax Funciton\n",
    "\n",
    "And then will be used the SoftMax Function,\n",
    "$$A = \\frac{e^{Z}}{\\sum_{n=1}^{N}{e^{Z_n}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How this is Translated to TensorFlow\n",
    "\n",
    "In order to create our model we need to use the functions of TensorFlow for everything, for example the multiplication of two arays is implemented with the funciton $tf.matmul()$ and the add of two arrays-tensors is implemented with this funciton $tf.add()$.\n",
    "\n",
    "So if we want to multiply two arrays and then ad the result with a new one $Z = X*W + b$, we need to use the above function with this order\n",
    "\n",
    "$$tf.add(tf.matmul(X, W),b)$$\n",
    "\n",
    "Finally we need to use the SoftMax fucntion so we need to use in the result this funciton $tf.nn.softmax()$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "Y = tf.nn.softmax(tf.add(tf.matmul(X, W),b))\n",
    "# placeholder for correct labels\n",
    "Y_ = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy \n",
    "\n",
    "Again we need to right the function of Cross Entropy into TensorFlow Code, first we will start with the classical function of Cost \n",
    "\n",
    "$$C = - \\sum_{k=1}^{m}Y^{real}*log(Y)$$ \n",
    "\n",
    "First we need to create the multiplication with the $log$\n",
    "\n",
    "$$Y^{real}*tf.log(Y)$$ \n",
    "\n",
    "Then we need to sum all this points and find the mean value of them so we will use the $tf.reduce sum()$ and $tf.reduce mean()$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(Y_ * tf.log(Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization - BackPropagation\n",
    "\n",
    "As we already mention in tensorflow you only need to take care of the forward propagation and cost function, everything elese will be taken care for you, this means that the only think that we need to do from now on is to specify how exactly the optimization will happen, so how exactly the weights will be updated-improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.0003)\n",
    "train_step = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model - Improve the Weights and Biasis\n",
    "\n",
    "At this point we need to Train the model and more specifically start the Session in order for the model to start training itself. So the first step is to right the command line \n",
    "\n",
    "$$sess = tf.Session()$$\n",
    "\n",
    "And from now on $sess$ the variable that rund the neural network.\n",
    "\n",
    "It is really important to initialize the variable again here just by writing $sess.run(init)$ and as you can see we use the init which is the name with the initalization of the varables.\n",
    "\n",
    "Then of course we need a for loop and here we make 1000 loops, this means that the neural network will improve itself 1000 time with differnt intput data every time.\n",
    "\n",
    "Finally you need to use the command line $sess.run()$ again but this time you will run it with the $train_step$ which is the optimization of the ANN and the input data of course the images and the labels\n",
    "\n",
    "$$sess.run(trainStep, feedDict=trainData)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "690.77496\n",
      "661.99756\n",
      "638.4052\n",
      "616.6336\n",
      "591.23047\n",
      "565.2551\n",
      "539.9378\n",
      "519.2126\n",
      "503.20352\n",
      "483.12067\n",
      "487.92548\n",
      "466.3634\n",
      "441.57397\n",
      "438.34857\n",
      "425.66394\n",
      "399.24072\n",
      "402.1453\n",
      "380.39307\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-1bf156fe4ffa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# ============ Show the Cost in every loop ===============\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mcross_entropy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[1;33m[\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# ============ Show the Cost in every loop ===============\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\milto\\Anaconda3\\envs\\tensorflow-sessions\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\milto\\Anaconda3\\envs\\tensorflow-sessions\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\milto\\Anaconda3\\envs\\tensorflow-sessions\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\milto\\Anaconda3\\envs\\tensorflow-sessions\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\milto\\Anaconda3\\envs\\tensorflow-sessions\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\milto\\Anaconda3\\envs\\tensorflow-sessions\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "Batch=300\n",
    "point=0\n",
    "for i in range(1000):\n",
    "    \n",
    "    train_data={X: Tr_Im[point:point+Batch,:], Y_:Ytr[point:point+Batch,:]} # Store the training data\n",
    "\n",
    "    # ============ Show the Cost in every loop ===============\n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(Y_ * tf.log(Y)))\n",
    "    [_,cross_entropy] = sess.run([train_step,cross_entropy],feed_dict=train_data)\n",
    "    print(cross_entropy)\n",
    "    # ============ Show the Cost in every loop ===============\n",
    "    \n",
    "    \n",
    "    #sess.run(train_step, feed_dict=train_data) # Just make the culculations\n",
    "    \n",
    "    \n",
    "    point = point+Batch # Use the new Batch of Images\n",
    "    if (point>60000):\n",
    "        point=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the Accuracy of the model \n",
    "\n",
    "First we need to find to find for every image if she had a correct prediction or not. This means that we will create a boolean array with False and True depending on if the result was correct or not.\n",
    "\n",
    "This can be done with two functions the first one is $tf.argmax()$ which culculates the position of the bigest number in the array $Y$. For this array Y:\n",
    "\n",
    "\\begin{equation*}\n",
    "Y =\n",
    "  \\begin{bmatrix}\n",
    "    0.2 &\n",
    "    0.3 &\n",
    "    0.05 &\n",
    "    \\dotsc &\n",
    "    0.001 \n",
    "  \\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "The result will be $1$, because the Neural Network predicts that the picture has 0.3 propability to be the number 1 and this propability is the bigest one.\n",
    "\n",
    "Then the funciton $tf.equal$ will be used in order to cuclulate if the real labels and the created ones from the neural network are the same.\n",
    "\n",
    "Now we need to enumarate this array, but in order to do that we need to change it from boolean to numbers 1 and 0. This can be done with the funciton $tf.cast()$ with the dataType $Float$.\n",
    "\n",
    "Finally we need to find how many images are correct VS how many images are wrong and for this specific table that hs only ones and zeros is the same to do the average of this aray. So we can use the function $tf.reduceMean$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.77\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(Y,1), tf.argmax(Y_,1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "print (sess.run(accuracy, feed_dict={X: Tr_Im[400:500,:], Y_:Ytr[400:500,:]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Results and Testing\n",
    "\n",
    "First we need to get the predected values of Y for every single of the 60000 Images. For that reason we will run our nural network with the whole data set and Y as atribute in order to get Y as a result \n",
    "\n",
    "$$Ytt = sess.run(Y, feedDict={X: Tr_Im, Y_:Ytr})$$\n",
    "\n",
    "Then we will just show the result for different idexes or even better for different images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.56491184 0.00325196 0.0193298  0.08850715 0.01268202 0.16916648\n",
      " 0.0334553  0.02155158 0.06596231 0.02118145]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Ytt = sess.run(Y, feed_dict={X: Tr_Im, Y_:Ytr})\n",
    "\n",
    "\n",
    "idx=81\n",
    "print(Ytt[idx,:])\n",
    "#print(sum(Ytt[idx,:])) # Because A2 is the propability to be something and the sumation of this propabilities is equal to 1\n",
    "print(Ytr[idx,:])\n",
    "\n",
    "print(np.argmax(Ytt[idx,:]))\n",
    "print(np.argmax(Ytr[idx,:]))\n",
    "imgplot = plt.imshow(train_images[idx,:,:], cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
